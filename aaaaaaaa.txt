




这是如假包换的PPYOLOE算法，咩酱在ncnn中对图片预处理时，与原版PPYOLOE一样，先将图片从BGR格式转成RGB格式，与原版PPYOLOE一样，用cv2.INTER_CUBIC方式将图片插值成640x640的大小，与原版PPYOLOE一样，使用相同的均值和标准差对图片进行归一化。确保了C++端和python端输入神经网络的图片张量是一样的。最后，ncnn的输出与miemiedetection的输出对比如下图所示：

右边是miemiedetection的输出，为ppyoloe_crn_s_300e_coco.pth这个模型预测的结果，miemiedetection根目录下输入

python tools/demo.py image -f exps/ppyoloe/ppyoloe_crn_s_300e_coco.py -c ppyoloe_crn_s_300e_coco.pth --path assets/000000000019.jpg --conf 0.15 --tsize 640 --save_result --device gpu

即可得到右边的结果。左边是ncnn相同的模型ppyoloe_crn_s_300e_coco的结果，ncnn的运算结果与pytorch有细微差别，影响不大。


# 如何实现

PPYOLOE并没有走“PPYOLOE -> onnx -> ncnn”这一条道路，巧妙地避开了onnx中间商赚差价。原因有二，其一，如果先转onnx，再转ncnn的话，PPYOLOE中有些层转onnx后得到的层不支持转到ncnn，比如b, _, h, w = feat.shape这句代码转onnx后会得到一个Shape层，转ncnn会报"Shape not supported yet!"；其二，不够高效，比如reg_dist = F.softmax(reg_dist, dim=1)这句代码，转onnx后会多出2个Transpose层在Softmax层前后，猜测可能是onnx不支持softmax在dim==1时的计算，所以先把dim==1的维先放到最后，再计算softmax，最后把维放回原来的位置。但是ncnn是支持softmax在dim==1时的计算的。所以咩酱选择了pytorch直接转ncnn，避开了onnx内鬼中间商。

但是咩酱也并没有选择pnnx，想自己开一个新坑，实现另一套pytorch直接转ncnn的方法。我读了一部分ncnn的源码，确保了自己对*.bin和*.param文件充分了解之后，封装了1个工具ncnn_utils，源码位于miemiedetection的mmdet/models/ncnn_utils.py，它支持写一次前向传播就能导出ncnn使用的*.bin和*.param文件（红字），你只需给每个pytorch层增加1个export_ncnn()方法，export_ncnn()方法几乎只要照抄farward()方法就能把模型导出到ncnn。以下是ncnn_utils工具的使用示例：


是不是很牛x？你只要照着farward()方法写，在export_ncnn()方法里用ncnn_utils的api写一次前向传播就能把pytorch模型导出到ncnn。
















